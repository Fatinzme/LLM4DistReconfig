{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d8ea94-c6db-4617-a598-7892a2546dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8bd97c-ec1f-47a3-8491-ab049353e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('/path/to/LLM-Reconfiguration/Dataset-Notebooks/utils'))\n",
    "\n",
    "from dataset_utils import prepare_train_data\n",
    "from model_utils import get_model, get_tokenizer\n",
    "from generation_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757257f-43c2-4f4b-a352-23b0a69ae43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "access_token = 'hf_token'\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6954012-6149-4aca-a2ae-cda26d520f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"/path/to/LLM-Reconfiguration/Dataset-Notebooks/train_files/train_33_69_84_nodes.csv\"\n",
    "model_id=\"/path/to/LLM-Reconfiguration/AutoTrain/llama3/model_name/checkpoint-26280\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db57f3-a3f4-492f-84b3-9ac081008156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(model_id)\n",
    "tokenizer = get_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4344e-892a-4c78-b527-d418cdfcd12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset, test_dataset = prepare_train_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef65611-4171-4c56-8876-0d205a244198",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f36bd-ecfb-488c-bff2-5d0fc8fb98fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =\"/path/to/LLM-Reconfiguration/AutoTrain/llama3/model_name/checkpoint-26280\"\n",
    "\n",
    "model = peft_merge_unload(model_id, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff0f496-bd7b-4fe5-919c-68c16a3ff685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8530e7-25aa-4b21-979d-aadac4e200f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 120000  # Adjust according to your model's capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb7d7a-0d4f-49f3-8e10-6dfb43eed1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = \"\"\n",
    "\n",
    "# generation_config = GenerationConfig(\n",
    "#       penalty_alpha=penalty_alpha,\n",
    "#       do_sample = do_sample,\n",
    "#       top_k=top_k,\n",
    "#       temperature=temperature,\n",
    "#       repetition_penalty=repetition_penalty,\n",
    "#       max_new_tokens=max_new_tokens,\n",
    "#       pad_token_id=tokenizer.eos_token_id, \n",
    "#       eos_token_id=tokenizer.eos_token_id  # Ensure EOS token is set - This is a new parameter so maybe it breaks the code.\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5cc51-2bfb-4500-adc8-90d180c235b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        # Get user input\n",
    "        user_input = input(\"You: \").strip()\n",
    "\n",
    "        # Check for exit commands before processing further\n",
    "        if user_input.lower() in ['quit', 'exit']:\n",
    "            print(\"Exiting the chat.\")\n",
    "            break  # Break out of the loop if user wants to quit\n",
    "        \n",
    "        conversation_history += f\"User: {user_input}\\n\"\n",
    "    \n",
    "        # Tokenize the conversation history\n",
    "        input_ids = tokenizer.encode(conversation_history, return_tensors='pt').to('cuda')\n",
    "    \n",
    "        # If the number of tokens exceeds the limit, truncate the beginning\n",
    "        if input_ids.size(-1) > max_tokens:\n",
    "            input_ids = input_ids[:, -max_tokens:]\n",
    "    \n",
    "        # Generate a response\n",
    "        outputs = model.generate(input_ids, \n",
    "                                 max_length=120000, \n",
    "                                 max_new_tokens = 1200,\n",
    "                                 do_sample=True, \n",
    "                                 top_k=5,\n",
    "                                 penalty_alpha=0.6,\n",
    "                                 temperature=0.5, \n",
    "                                 repetition_penalty=1.2,\n",
    "                                 pad_token_id=tokenizer.eos_token_id, \n",
    "                                 eos_token_id=tokenizer.eos_token_id )\n",
    "    \n",
    "        # Decode and print the model's response\n",
    "        response = tokenizer.decode(outputs[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "        conversation_history += f\"Model: {response}\\n\"\n",
    "    \n",
    "        print(f\"Model: {response}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Handle manual interruption (Ctrl+C)\n",
    "        print(\"Chat interrupted manually. Exiting...\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb4d78-051d-4ec9-903b-fce7e5aa226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57275b4-0683-42cd-a18f-bc9ce0f68db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc03cf-dfe4-477b-acf0-ac2a42d3c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51c119-368a-46b7-aaf1-915b40924564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5d112-b808-44a3-9227-3fe18f7188fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
